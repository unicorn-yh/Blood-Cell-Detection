/content/drive/MyDrive/BloodcellDetection/BCCD/YOLOv6
Namespace(batch_size=32, conf_thres=0.03, config_file='', data='data/bccd.yaml', device='0', do_coco_metric=True, do_pr_metric=False, eval_config_file='./configs/experiment/eval_640_repro.py', force_no_pad=False, half=False, img_size=640, iou_thres=0.65, letterbox_return_int=False, name='exp', not_infer_on_rect=False, plot_confusion_matrix=False, plot_curve=True, reproduce_640_eval=False, save_dir='runs/val/', scale_exact=False, task='val', test_load_size=640, verbose=False, weights='/content/drive/MyDrive/BloodcellDetection/BCCD/YOLOv6/runs/train/exp7/weights/best_ckpt.pt')
Loading checkpoint from /content/drive/MyDrive/BloodcellDetection/BCCD/YOLOv6/runs/train/exp7/weights/best_ckpt.pt

Fusing model...
/usr/local/lib/python3.7/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Switch model to deploy modality.
Model Summary: Params: 17.19M, Gflops: 44.07
Val: Checking formats of labels with 2 process(es): 
87 label(s) found, 0 label(s) missing, 0 label(s) empty, 0 invalid label files: 100% 87/87 [00:20<00:00,  4.27it/s]
Convert to COCO format
100% 87/87 [00:00<00:00, 18436.97it/s]
Convert to COCO format finished. Resutls saved in /content/drive/MyDrive/BloodcellDetection/BCCD/annotations/instances_val.json
Val: Final numbers of valid images: 87/ labels: 87. 
20.6s for dataset initialization.
Inferencing model in val datasets.: 100% 3/3 [00:03<00:00,  1.02s/it]

Evaluating speed.
Average pre-process time: 0.15 ms
Average inference time: 7.53 ms
Average NMS time: 4.13 ms

Evaluating mAP by pycocotools.
Saving runs/val/exp2/predictions.json...
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.18s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.03s).
Accumulating evaluation results...
DONE (t=0.13s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.567
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.863
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.608
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.123
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.680
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.382
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.608
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.688
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.178
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.621
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.778
Results saved to runs/val/exp2



